{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13477706,"sourceType":"datasetVersion","datasetId":8556482}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Setup and Installation","metadata":{}},{"cell_type":"code","source":"!pip install librosa scikit-learn matplotlib numpy pandas noisereduce tensorflow shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:58:04.969189Z","iopub.execute_input":"2025-10-23T15:58:04.969353Z","iopub.status.idle":"2025-10-23T15:58:09.897137Z","shell.execute_reply.started":"2025-10-23T15:58:04.969338Z","shell.execute_reply":"2025-10-23T15:58:09.896195Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting noisereduce\n  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.15.0)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from noisereduce) (4.67.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.75.1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.1.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.4.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\nInstalling collected packages: noisereduce\nSuccessfully installed noisereduce-3.0.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Cell 1.5","metadata":{}},{"cell_type":"code","source":"# Cell 1.5: Updated - No unzip needed since files are already extracted\nimport os\nimport shutil\n\n# Paths to uploaded files\ndataset_path = '/kaggle/input/train-audio'\ndata_dir = '/kaggle/working/violin-emotion-analysis/data'\ncsv_path = '/kaggle/input/train-audio/emotion_labels.csv'\n\n# Create the working directory\nos.makedirs(data_dir, exist_ok=True)\n\n# Copy all audio files from the dataset to our working directory\naudio_source_dir = '/kaggle/input/train-audio/audio'\nif os.path.exists(audio_source_dir):\n    # Copy all WAV files to our working directory\n    for file_name in os.listdir(audio_source_dir):\n        if file_name.endswith('.wav'):\n            source_path = os.path.join(audio_source_dir, file_name)\n            dest_path = os.path.join(data_dir, file_name)\n            shutil.copy2(source_path, dest_path)\n    \n    print(f\"Copied audio files to working directory\")\nelse:\n    print(\"Audio folder not found!\")\n\n# Check folder structure\nprint(\"\\nData directory structure:\")\nfor root, dirs, files in os.walk(data_dir):\n    level = root.replace(data_dir, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    subindent = ' ' * 2 * (level + 1)\n    for f in files:\n        print(f\"{subindent}{f}\")\n\n# Check if CSV exists\nif not os.path.exists(csv_path):\n    print(\"\\nWARNING: 'emotion_labels.csv' not found!\")\nelse:\n    print(f\"\\nCSV file found: {csv_path}\")\n    # Also copy CSV to working directory for consistency\n    shutil.copy2(csv_path, '/kaggle/working/violin-emotion-analysis/emotion_labels.csv')\n    print(\"CSV copied to working directory\")\n\nprint(f\"\\nTotal audio files ready for processing: {len([f for f in os.listdir(data_dir) if f.endswith('.wav')])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:58:13.630093Z","iopub.execute_input":"2025-10-23T15:58:13.630370Z","iopub.status.idle":"2025-10-23T15:58:18.093085Z","shell.execute_reply.started":"2025-10-23T15:58:13.630341Z","shell.execute_reply":"2025-10-23T15:58:18.092465Z"}},"outputs":[{"name":"stdout","text":"Copied audio files to working directory\n\nData directory structure:\ndata/\n  s2.wav\n  h3.wav\n  h11.wav\n  s10.wav\n  h23.wav\n  s15.wav\n  s27.wav\n  h15.wav\n  s22.wav\n  s18.wav\n  h1.wav\n  a20.wav\n  a7.wav\n  s4.wav\n  a3.wav\n  s1.wav\n  a15.wav\n  s11.wav\n  a2.wav\n  s17.wav\n  h16.wav\n  s20.wav\n  h13.wav\n  a30.wav\n  a22.wav\n  h4.wav\n  a13.wav\n  s29.wav\n  h24.wav\n  a17.wav\n  a6.wav\n  a1.wav\n  s23.wav\n  h6.wav\n  s16.wav\n  h7.wav\n  h17.wav\n  a12.wav\n  h25.wav\n  a18.wav\n  a9.wav\n  s28.wav\n  h29.wav\n  s7.wav\n  h14.wav\n  a25.wav\n  h20.wav\n  s14.wav\n  s24.wav\n  s30.wav\n  s26.wav\n  s3.wav\n  a24.wav\n  h12.wav\n  s13.wav\n  h2.wav\n  h9.wav\n  h22.wav\n  a11.wav\n  a29.wav\n  h19.wav\n  h21.wav\n  a8.wav\n  h10.wav\n  s6.wav\n  a16.wav\n  a4.wav\n  a19.wav\n  h30.wav\n  s9.wav\n  s19.wav\n  a21.wav\n  h5.wav\n  a14.wav\n  a26.wav\n  a28.wav\n  a23.wav\n  h18.wav\n  s21.wav\n  a5.wav\n  h27.wav\n  s12.wav\n  s5.wav\n  a10.wav\n  h8.wav\n  h28.wav\n  h26.wav\n  s8.wav\n  s25.wav\n  a27.wav\n\nCSV file found: /kaggle/input/train-audio/emotion_labels.csv\nCSV copied to working directory\n\nTotal audio files ready for processing: 90\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Cell 2: Imports and Updated Utility Functions","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# Cell 2: Imports and Advanced Preprocessing with Noise Handling\n# ===============================================================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Attention, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport noisereduce as nr\nimport shap\nimport joblib\nimport scipy.signal as signal\nfrom tqdm import tqdm\nimport warnings\nimport soundfile as sf\nwarnings.filterwarnings('ignore')\n\n# Create directories\nos.makedirs('/kaggle/working/violin-emotion-analysis/data', exist_ok=True)\nos.makedirs('/kaggle/working/violin-emotion-analysis/models', exist_ok=True)\n\n# ===============================================================\n# 1️⃣ Feature Extraction Helper (Temporal + Cleaned Audio)\n# ===============================================================\n\ndef extract_temporal_features(file_path, sr=44100, frame_length=2048, hop_length=512, window_size=3.0, hop_time=1.5):\n    \"\"\"\n    Extract richer emotion-aware features from violin recordings.\n    Captures pitch, tonality, articulation, dynamics, dissonance, and temporal evolution.\n    \"\"\"\n    y, sr = librosa.load(file_path, sr=sr)\n    y = nr.reduce_noise(y=y, sr=sr)\n\n    total_duration = librosa.get_duration(y=y, sr=sr)\n    step = int(hop_time * sr)\n    window = int(window_size * sr)\n\n    feature_sequences = []\n    for start in range(0, len(y) - window, step):\n        segment = y[start:start + window]\n        segment_features = []\n\n        # === Core tone features ===\n        mfccs = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n        mfccs_delta = librosa.feature.delta(mfccs)\n        chroma = librosa.feature.chroma_stft(y=segment, sr=sr)\n        chroma_delta = librosa.feature.delta(chroma)\n        spectral_contrast = librosa.feature.spectral_contrast(y=segment, sr=sr)\n\n        # === Expression features (temporal dynamics) ===\n        rms = librosa.feature.rms(y=segment)  # energy dynamics\n        zcr = librosa.feature.zero_crossing_rate(y=segment)  # articulation\n        centroid = librosa.feature.spectral_centroid(y=segment, sr=sr)\n        bandwidth = librosa.feature.spectral_bandwidth(y=segment, sr=sr)\n        flatness = librosa.feature.spectral_flatness(y=segment)\n        tempo, _ = librosa.beat.beat_track(y=segment, sr=sr)\n        harmony = librosa.feature.tonnetz(y=librosa.effects.harmonic(segment), sr=sr)\n\n        # === Statistical + temporal summarization per segment ===\n        # For RMS and ZCR, include their mean, std, and curve (downsampled)\n        curve_len = 20  # downsample curve to 20 points\n        rms_curve = np.interp(np.linspace(0, rms.shape[1]-1, curve_len), np.arange(rms.shape[1]), rms[0])\n        zcr_curve = np.interp(np.linspace(0, zcr.shape[1]-1, curve_len), np.arange(zcr.shape[1]), zcr[0])\n\n        feature_vec = np.hstack([\n            np.mean(mfccs, axis=1), np.std(mfccs, axis=1),\n            np.mean(mfccs_delta, axis=1), np.std(mfccs_delta, axis=1),\n            np.mean(chroma, axis=1), np.mean(chroma_delta, axis=1),\n            np.mean(spectral_contrast, axis=1),\n            np.mean(centroid), np.std(centroid),\n            np.mean(bandwidth),\n            np.mean(flatness),\n            tempo if not np.isnan(tempo) else 0,\n            np.mean(harmony, axis=1),\n            np.mean(rms), np.std(rms), rms_curve,  # include RMS curve\n            np.mean(zcr), np.std(zcr), zcr_curve   # include ZCR curve\n        ])\n\n        feature_sequences.append(feature_vec)\n\n    return np.array(feature_sequences)\n\n# ===============================================================\n# 2️⃣ Data Augmentation Helper\n# ===============================================================\ndef augment_audio(y, sr):\n    \"\"\"\n    Create slightly altered versions of the same clip to make the model more robust.\n    Includes small pitch shifts and speed changes.\n    \"\"\"\n    augmented = []\n\n    # Pitch shifts: ±2 semitones\n    for n_steps in [-2, 2]:\n        augmented.append(librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps))\n\n    # Time stretching using resampling (since librosa.effects.time_stretch expects a spectrogram now)\n    for rate in [0.9, 1.1]:\n        new_length = int(len(y) / rate)\n        augmented.append(librosa.resample(y, orig_sr=sr, target_sr=int(sr * rate))[:new_length])\n\n    return augmented\n\n\n# ===============================================================\n# 3️⃣ Create Temporal Dataset (with Augmented Samples)\n# ===============================================================\ndef create_temporal_dataset(data_dir, emotion_csv_path, augment=True):\n    \"\"\"\n    Creates dataset with temporal features and soft emotion labels.\n    The CSV must include: filename, emotion, and soft label columns (e.g., happy, sad, angry).\n    \"\"\"\n    annotations = pd.read_csv(emotion_csv_path)\n    X, y_soft = [], []\n\n    for _, row in tqdm(annotations.iterrows(), total=len(annotations), desc=\"Creating dataset\"):\n        file_path = os.path.join(data_dir, row['filename'])\n        if not os.path.exists(file_path):\n            continue\n\n        # Extract base features\n        base_features = extract_temporal_features(file_path)\n        X.append(base_features)\n        y_soft.append(row[4:].values.astype(float))  # ← soft emotion probabilities\n        \n        # --- Optional Augmentation ---\n        if augment:\n            y, sr = librosa.load(file_path, sr=44100)\n            y = nr.reduce_noise(y=y, sr=sr)\n            y = librosa.util.normalize(y)\n            for aug_y in augment_audio(y, sr):\n                # Save augmented version's features\n                temp_file = \"augmented_temp.wav\"\n                sf.write(temp_file, aug_y, sr)\n                aug_features = extract_temporal_features(temp_file)\n                X.append(aug_features)\n                y_soft.append(row[4:].values.astype(float))\n                os.remove(temp_file)\n\n    # Pad sequences for LSTM\n    max_len = max(x.shape[0] for x in X)\n    num_features = X[0].shape[1]\n    X_padded = np.zeros((len(X), max_len, num_features))\n    for i, seq in enumerate(X):\n        X_padded[i, :seq.shape[0], :] = seq\n\n    return np.array(X_padded), np.array(y_soft)\n\nprint(\"✅ Advanced preprocessing module loaded (with noise reduction, normalization, and augmentation).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:58:22.920134Z","iopub.execute_input":"2025-10-23T15:58:22.920715Z","iopub.status.idle":"2025-10-23T15:58:43.911202Z","shell.execute_reply.started":"2025-10-23T15:58:22.920691Z","shell.execute_reply":"2025-10-23T15:58:43.910388Z"}},"outputs":[{"name":"stderr","text":"2025-10-23 15:58:25.751252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761235105.966688      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761235106.037936      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ Advanced preprocessing module loaded (with noise reduction, normalization, and augmentation).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Cell 3: Data Preparation","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/working/violin-emotion-analysis/data'\nemotion_csv_path = '/kaggle/working/violin-emotion-analysis/emotion_labels.csv'  # <-- upload your CSV here\n\nif not os.path.exists(emotion_csv_path):\n    print(\"Please upload 'emotion_labels.csv' with soft emotion probabilities.\")\nelse:\n    X, y_soft = create_temporal_dataset(data_dir, emotion_csv_path)\n    print(f\"Loaded {X.shape[0]} samples with {X.shape[2]} features each and {y_soft.shape[1]} soft emotion dimensions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T15:59:10.078281Z","iopub.execute_input":"2025-10-23T15:59:10.078882Z","iopub.status.idle":"2025-10-23T16:32:20.438797Z","shell.execute_reply.started":"2025-10-23T15:59:10.078851Z","shell.execute_reply":"2025-10-23T16:32:20.437959Z"}},"outputs":[{"name":"stderr","text":"Creating dataset: 100%|██████████| 90/90 [33:10<00:00, 22.11s/it] ","output_type":"stream"},{"name":"stdout","text":"Loaded 450 samples with 138 features each and 3 soft emotion dimensions.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Cell 4: Model Training (LSTM + Random Forest)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, LayerNormalization, Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\ndef build_hybrid_model(input_shape, output_dim):\n    \"\"\"\n    Hybrid model: Bidirectional LSTM for temporal emotion cues\n    + Random Forest for interpretability and ensemble robustness.\n    \"\"\"\n    # --- LSTM Branch (Temporal Learning) ---\n    inp = Input(shape=input_shape)\n    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n    x = LayerNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = Dropout(0.3)(x)\n    lstm_output = Dense(64, activation='relu')(x)\n\n    # --- Output for fusion ---\n    out = Dense(output_dim, activation='softmax', name=\"lstm_output\")(lstm_output)\n    model = Model(inputs=inp, outputs=out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n# --- Prepare data ---\nX_train, X_test, y_train, y_test = train_test_split(X, y_soft, test_size=0.2, random_state=42)\n\n# --- Build and train LSTM ---\ninput_shape = (X_train.shape[1], X_train.shape[2])\noutput_dim = y_train.shape[1]\n\nlstm_model = build_hybrid_model(input_shape, output_dim)\nearly_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n\nhistory = lstm_model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=80,\n    batch_size=16,\n    callbacks=[early_stop],\n    verbose=1\n)\n\n# --- Extract embeddings from the LSTM layer before softmax ---\nembedding_model = Model(inputs=lstm_model.input,\n                        outputs=lstm_model.get_layer(\"lstm_output\").output)\n\ntrain_embeddings = embedding_model.predict(X_train)\ntest_embeddings = embedding_model.predict(X_test)\n\n# --- Random Forest for interpretability ---\nrf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\nrf_model.fit(train_embeddings, y_train)\n\n# --- Weighted fusion of predictions ---\nlstm_preds = lstm_model.predict(X_test)\nrf_preds = rf_model.predict(test_embeddings)\nfused_preds = 0.7 * lstm_preds + 0.3 * rf_preds\n\n# --- Evaluate ---\nr2 = r2_score(y_test, fused_preds)\nprint(f\"Hybrid Model R² Score: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step - accuracy: 0.5414 - loss: 0.9601 - val_accuracy: 0.6667 - val_loss: 0.7915\nEpoch 2/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7962 - loss: 0.5623 - val_accuracy: 0.8472 - val_loss: 0.4641\nEpoch 3/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8601 - loss: 0.4052 - val_accuracy: 0.9028 - val_loss: 0.3081\nEpoch 4/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9362 - loss: 0.1951 - val_accuracy: 0.9028 - val_loss: 0.3387\nEpoch 5/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9160 - loss: 0.2178 - val_accuracy: 0.9167 - val_loss: 0.2630\nEpoch 6/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9118 - loss: 0.2318 - val_accuracy: 0.9306 - val_loss: 0.2894\nEpoch 7/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8946 - loss: 0.2664 - val_accuracy: 0.8472 - val_loss: 0.3780\nEpoch 8/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9210 - loss: 0.2097 - val_accuracy: 0.9167 - val_loss: 0.2447\nEpoch 9/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9568 - loss: 0.1513 - val_accuracy: 0.9306 - val_loss: 0.2248\nEpoch 10/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9476 - loss: 0.1459 - val_accuracy: 0.9306 - val_loss: 0.2132\nEpoch 11/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9636 - loss: 0.1405 - val_accuracy: 0.9444 - val_loss: 0.2610\nEpoch 12/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9479 - loss: 0.1270 - val_accuracy: 0.9167 - val_loss: 0.2200\nEpoch 13/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9676 - loss: 0.0814 - val_accuracy: 0.9306 - val_loss: 0.1832\nEpoch 14/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9352 - loss: 0.1264 - val_accuracy: 0.8889 - val_loss: 0.2704\nEpoch 15/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9461 - loss: 0.1687 - val_accuracy: 0.8750 - val_loss: 0.3481\nEpoch 16/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9150 - loss: 0.1850 - val_accuracy: 0.9306 - val_loss: 0.1884\nEpoch 17/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9697 - loss: 0.0714 - val_accuracy: 0.9167 - val_loss: 0.2546\nEpoch 18/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9529 - loss: 0.1179 - val_accuracy: 0.9444 - val_loss: 0.1982\nEpoch 19/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9613 - loss: 0.0900 - val_accuracy: 0.9583 - val_loss: 0.1722\nEpoch 20/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9833 - loss: 0.0631 - val_accuracy: 0.9583 - val_loss: 0.1880\nEpoch 21/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9766 - loss: 0.0485 - val_accuracy: 0.9306 - val_loss: 0.2289\nEpoch 22/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9551 - loss: 0.0824 - val_accuracy: 0.9583 - val_loss: 0.1810\nEpoch 23/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9550 - loss: 0.0886 - val_accuracy: 0.9583 - val_loss: 0.1535\nEpoch 24/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9714 - loss: 0.0743 - val_accuracy: 0.9444 - val_loss: 0.2053\nEpoch 25/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9739 - loss: 0.0599 - val_accuracy: 0.9583 - val_loss: 0.1526\nEpoch 26/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9785 - loss: 0.0671 - val_accuracy: 0.9583 - val_loss: 0.2410\nEpoch 27/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9643 - loss: 0.0915 - val_accuracy: 0.9444 - val_loss: 0.2804\nEpoch 28/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9718 - loss: 0.0828 - val_accuracy: 0.9583 - val_loss: 0.2148\nEpoch 29/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9537 - loss: 0.1723 - val_accuracy: 0.9583 - val_loss: 0.2089\nEpoch 30/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9517 - loss: 0.1071 - val_accuracy: 0.9028 - val_loss: 0.3086\nEpoch 31/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9532 - loss: 0.1483 - val_accuracy: 0.9583 - val_loss: 0.1856\nEpoch 32/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9046 - loss: 0.1905 - val_accuracy: 0.7778 - val_loss: 0.5821\nEpoch 33/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9020 - loss: 0.2827 - val_accuracy: 0.9167 - val_loss: 0.2711\nEpoch 34/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9373 - loss: 0.1867 - val_accuracy: 0.9167 - val_loss: 0.3323\nEpoch 35/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9698 - loss: 0.1121 - val_accuracy: 0.9444 - val_loss: 0.1643\nEpoch 36/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9673 - loss: 0.0597 - val_accuracy: 0.9583 - val_loss: 0.1715\nEpoch 37/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9766 - loss: 0.0663 - val_accuracy: 0.8889 - val_loss: 0.2523\nEpoch 38/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9039 - loss: 0.2644 - val_accuracy: 0.9028 - val_loss: 0.2641\nEpoch 39/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8971 - loss: 0.2736 - val_accuracy: 0.9583 - val_loss: 0.2171\nEpoch 40/80\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9011 - loss: 0.2194 - val_accuracy: 0.9583 - val_loss: 0.1785\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step\nHybrid Model R² Score: 0.8429\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Cell 5: Explainability and Visualization","metadata":{}},{"cell_type":"code","source":"# Cell 5: Enhanced SHAP Analysis with Emotion-Specific Features\nprint(\"🎵 SHAP Analysis - Feature Importance by Emotion\")\n\n# Get emotion labels from your CSV\nemotion_labels = ['Happy', 'Sad', 'Angry']\n\n# Create SHAP explainer\nexplainer = shap.Explainer(rf_model, X_train_flat[:100])  # Use first 100 training samples\nshap_values = explainer(X_test_flat[:50])\n\nprint(\"🔍 Analyzing feature importance for each emotion...\")\n\n# Create separate plots for each emotion\nfor emotion_idx, emotion_name in enumerate(emotion_labels):\n    print(f\"\\n🎻 {emotion_name.upper()} EMOTION:\")\n    print(\"=\"*40)\n    \n    # Get SHAP values for this specific emotion\n    emotion_shap_values = shap_values.values[:, :, emotion_idx]\n    \n    # Get top 10 most important features for this emotion\n    feature_importance = np.mean(np.abs(emotion_shap_values), axis=0)\n    top_feature_indices = np.argsort(feature_importance)[-10:][::-1]  # Top 10\n    \n    print(f\"Top features that predict {emotion_name}:\")\n    for i, idx in enumerate(top_feature_indices):\n        importance = feature_importance[idx]\n        print(f\"  {i+1}. Feature {idx}: {importance:.4f}\")\n    \n    # Create feature names based on what we know about the extraction\n    feature_categories = []\n    for i in range(138):  # ~138 features now (we added more)\n        if i < 26:  # First 26 are MFCCs (13 mean + 13 std)\n            if i < 13:\n                feature_categories.append(f\"MFCC_{i+1}_mean\")\n            else:\n                feature_categories.append(f\"MFCC_{i-12}_std\")\n        elif i < 38:  # Next 12 are Chroma (mean only)\n            feature_categories.append(f\"Chroma_{i-25}\")\n        elif i < 45:  # Next 7 are Spectral Contrast\n            feature_categories.append(f\"Spectral_Contrast_{i-37}\")\n        elif i == 45:\n            feature_categories.append(\"RMS_energy\")\n        elif i == 46:\n            feature_categories.append(\"Zero_Crossing_Rate\")\n        elif i == 47:\n            feature_categories.append(\"Spectral_Centroid\")\n        elif i == 48:\n            feature_categories.append(\"Spectral_Bandwidth\")\n        else:\n            feature_categories.append(f\"Feature_{i}\")\n    \n    print(f\"\\n📊 Named features for {emotion_name}:\")\n    for i, idx in enumerate(top_feature_indices[:5]):  # Top 5 with names\n        feature_name = feature_categories[idx]\n        importance = feature_importance[idx]\n        print(f\"  {i+1}. {feature_name}: {importance:.4f}\")\n\n# Create the main SHAP summary plot\nprint(\"\\n📈 Generating main SHAP summary plot...\")\nshap.summary_plot(shap_values, X_test_flat[:50], feature_names=feature_categories, show=False)\nplt.title(\"SHAP Feature Importance - All Emotions\")\nplt.tight_layout()\nplt.show()\n\n# Additional: Force plot for a single prediction to see detailed reasoning\nprint(\"\\n🔬 Detailed prediction breakdown for first test sample:\")\nshap.force_plot(explainer.expected_value[0], shap_values.values[0,:,0], X_test_flat[0,:], \n                feature_names=feature_categories, matplotlib=True, show=False)\nplt.title(f\"Prediction Breakdown: {emotion_labels[np.argmax(y_test_soft[0])]} → {emotion_labels[rf_model.predict(X_test_flat[0:1])[0]]}\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Simulation on New Audio Files","metadata":{}},{"cell_type":"code","source":"new_audio_dir = '/kaggle/working/violin-emotion-analysis/new_audio'  # put new audios here\nemotion_labels = pd.read_csv(emotion_csv_path).columns[3:].tolist()  # same soft labels as training\n\nnew_files = [f for f in os.listdir(new_audio_dir) if f.endswith('.wav')]\nprint(f\"Found {len(new_files)} new audio files for prediction.\")\n\nfor f in new_files:\n    path = os.path.join(new_audio_dir, f)\n    \n    # Extract features\n    features = extract_temporal_features(path)\n    \n    # Pad sequence to match LSTM input\n    padded = np.zeros((1, X_train.shape[1], X_train.shape[2]))\n    padded[0, :features.shape[0], :] = features\n    \n    # LSTM prediction\n    lstm_probs = lstm_model.predict(padded)\n    \n    # RF prediction\n    rf_probs = rf_model.predict_proba(np.mean(padded, axis=1))\n    rf_probs = np.array([np.pad(p, (0, len(emotion_labels) - len(p))) for p in rf_probs])\n    \n    # Hybrid fusion\n    hybrid_probs = 0.6*lstm_probs + 0.4*rf_probs\n    \n    # Perceptual smoothing\n    smoothed = np.convolve(np.mean(hybrid_probs, axis=0), np.ones(3)/3, mode='same')\n    \n    # Plot results\n    plt.figure(figsize=(8, 5))\n    plt.bar(emotion_labels, smoothed)\n    plt.title(f\"Predicted Emotions for {f}\")\n    plt.ylabel(\"Probability\")\n    plt.ylim(0,1)\n    plt.show()\n    \n    # Print prediction\n    pred_emotion = emotion_labels[np.argmax(smoothed)]\n    print(f\"Audio: {f} → Predicted Emotion: {pred_emotion}\")\n    print(\"Probability per emotion:\")\n    for label, prob in zip(emotion_labels, smoothed):\n        print(f\"  {label}: {prob:.3f}\")\n    print(\"\\n\" + \"-\"*40 + \"\\n\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}